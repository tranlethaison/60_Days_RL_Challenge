{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "cifar_10_super_convergence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tranlethaison/60_Days_RL_Challenge/blob/master/super_convergence/cifar_10_super_convergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKMuMH9Rn-gg"
      },
      "source": [
        "# Summary\n",
        "Train a model within 18 epochs, to at least 0.94 validation accuracy, on Cifar10 dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsj4CN7pPeQZ"
      },
      "source": [
        "# References\n",
        "- https://www.fast.ai/2018/07/02/adam-weight-decay/\n",
        "- https://keras.io/guides/transfer_learning/\n",
        "- https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/\n",
        "- https://blog.tensorflow.org/2020/05/bigtransfer-bit-state-of-art-transfer-learning-computer-vision.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLNLLwv3nwAN"
      },
      "source": [
        "# To-do\n",
        "- Try https://github.com/szagoruyko/wide-residual-networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3Xk_NdNQi2L"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hpKwQjXdsQp",
        "trusted": true,
        "_kg_hide-output": false,
        "outputId": "47db20c9-76ca-484a-b413-652d27d7da6f"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "plotly\n",
        "snoop\n",
        "\n",
        "# tensorflow\n",
        "tensorboard_plugin_profile\n",
        "tensorflow_addons\n",
        "tensorflow_hub"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0K1NcPwQD4W",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "!pip install -qUr requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlDHaWgqQWwV",
        "trusted": true,
        "scrolled": false,
        "outputId": "e0b715a9-f26c-4a1d-d9a3-649feeb8fdf9"
      },
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "import enum\n",
        "\n",
        "import numpy as np\n",
        "# import numba\n",
        "# from numba import njit, prange\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras as tk\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.express as px\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import snoop\n",
        "snoop.install()\n",
        "\n",
        "%load_ext tensorboard\n",
        "%load_ext snoop\n",
        "\n",
        "\n",
        "pp(tf.__version__)\n",
        "pp(tfa.__version__)\n",
        "pp(hub.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "04:31:47.87 LOG:\n",
            "04:31:47.88 .... tf.__version__ = '2.3.1'\n",
            "04:31:47.88 LOG:\n",
            "04:31:47.89 .... tfa.__version__ = '0.11.2'\n",
            "04:31:47.89 LOG:\n",
            "04:31:47.89 .... hub.__version__ = '0.10.0'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.10.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlV476Qo03xF",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "# Use these if run into Conv2D error \n",
        "# gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "# for gpu in gpus:\n",
        "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
        "# gpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSWOAjLhQ6er"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3iYu_P8Sqb1",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "ds_name = \"cifar-10\"\n",
        "\n",
        "# Behaviors\n",
        "do_make_dataset = True  #@param {type:\"boolean\"}\n",
        "do_use_mixed_precision = False  #@param {type:\"boolean\"}\n",
        "do_augmentation = True  #@param {type:\"boolean\"}\n",
        "do_find_lr = False  #@param {type:\"boolean\"}\n",
        "do_train = True  #@param {type:\"boolean\"}\n",
        "do_load_model = False  #@param {type:\"boolean\"}\n",
        "\n",
        "on_colab = True  #@param {type:\"boolean\"}\n",
        "on_kaggle = False  #@param {type:\"boolean\"}\n",
        "assert (on_colab and on_kaggle) == False\n",
        "# << Behaviors\n",
        "\n",
        "# Directory\n",
        "home_dir = os.path.expanduser(\"~\")\n",
        "now_str = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "if on_colab:\n",
        "    work_dp = r\"/content/drive/My Drive/AITrainingRecipe/super_convergence\"\n",
        "elif on_kaggle:\n",
        "    work_dp = r\"/kaggle/working/AITrainingRecipe/super_convergence\"\n",
        "else:\n",
        "    work_dp = None\n",
        "    \n",
        "if not work_dp is None:\n",
        "    os.makedirs(work_dp, exist_ok=True)\n",
        "    os.chdir(work_dp)\n",
        "# !pwd && ls -lh && du -h\n",
        "\n",
        "dataset_dp = os.path.join(home_dir, \"datasets\", ds_name)\n",
        "os.makedirs(dataset_dp, exist_ok=True)\n",
        "\n",
        "tfhub_cache_dir = os.path.join(home_dir, \"tfhub_modules\")\n",
        "os.environ[\"TFHUB_CACHE_DIR\"] = tfhub_cache_dir\n",
        "os.makedirs(tfhub_cache_dir, exist_ok=True)\n",
        "\n",
        "lr_find_result_dir = os.path.join(\"lr_find_result\")\n",
        "os.makedirs(lr_find_result_dir, exist_ok=True)\n",
        "# << Directory\n",
        "\n",
        "# Dataset info\n",
        "# input_shape = [32, 32, 3]\n",
        "# input_shape = [71, 71, 3]\n",
        "# input_shape = [75, 75, 3]\n",
        "# input_shape = [96, 96, 3]\n",
        "input_shape = [128, 128, 3]\n",
        "# input_shape = [224, 224, 3]\n",
        "n_classes = 10\n",
        "# << Dataset info\n",
        "\n",
        "# Training\n",
        "# GPU Tensor Cores (XLA) requires batch_size to be a multiple of 8\n",
        "batch_size = 128  # 128 256 512 1024\n",
        "n_epochs = 18\n",
        "# << Training"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JilVdjg28BO"
      },
      "source": [
        "# Training Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ulr4i6-25Iq",
        "trusted": true,
        "scrolled": false,
        "outputId": "fe49c422-4996-4707-ade9-cc316551ce64"
      },
      "source": [
        "# Input prefetch.\n",
        "!lscpu -e \n",
        "\n",
        "# Number of CPU threads (nproc --all)\n",
        "workers = int(subprocess.check_output(\"nproc --all\", shell=True))\n",
        "\n",
        "prefetch_cfg = dict(\n",
        "    max_queue_size=10,\n",
        "    workers=workers,  \n",
        ")\n",
        "pp(prefetch_cfg)\n",
        "# << Input prefetch.\n",
        "\n",
        "# Reduce 'Kernel Launch' time.\n",
        "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
        "\n",
        "# Mixed precision (use of both 16-bit and 32-bit).\n",
        "# If using \"Mixed precision\", remember to cast output of last layer to \"float32\" for numeric stability.\n",
        "# For GPU: \"mixed_float16\"; for TPU: \"mixed_bfloat16\"\n",
        "if do_use_mixed_precision:\n",
        "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_policy(policy)\n",
        "\n",
        "    pp(policy.compute_dtype)\n",
        "    pp(policy.variable_dtype)\n",
        "    pp(policy.loss_scale)\n",
        "!nvidia-smi -L\n",
        "# << Mixed precision.\n",
        "\n",
        "\n",
        "def reset_env():\n",
        "    \"\"\"Call this before creating new model.\"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.config.optimizer.set_jit(True)  # Enable XLA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE\r\n",
            "0   0    0      0    0:0:0:0       yes\r\n",
            "1   0    0      0    0:0:0:0       yes\r\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "04:32:49.67 LOG:\n",
            "04:32:49.68 .... prefetch_cfg = {'max_queue_size': 10, 'workers': 2}\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-72982998-4d8c-0b67-3619-ed640a491dfa)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9OStAR5RBEm"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "919L2_M0Q_Kk",
        "trusted": true,
        "scrolled": false,
        "outputId": "4cda37a4-ee85-4d38-8e1b-1a9a8ee164b4"
      },
      "source": [
        "%%time\n",
        "# https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        d = pickle.load(fo, encoding='bytes')\n",
        "    return d\n",
        "\n",
        "\n",
        "def data_to_img(data):\n",
        "    data = np.array_split(data, 3, axis=0)\n",
        "    data = [np.array_split(channel, 32, axis=0) for channel in data]\n",
        "    data = np.stack(data, axis=-1)\n",
        "    return data\n",
        "\n",
        "\n",
        "def save_images(images, dir, labels, filenames, target_size=None, do_override=False):\n",
        "    # Resize image\n",
        "    image_size = images.shape[1:3]\n",
        "    if target_size is not None and list(target_size) != image_size:\n",
        "        target_height, target_width = target_size\n",
        "        images = tf.image.resize_with_pad(images, target_height, target_width).numpy()\n",
        "        print(\"Resized images from {} to {}.\".format(image_size, target_size))\n",
        "\n",
        "    for image, label, filename in tqdm(zip(images, labels, filenames)):\n",
        "        image_fp = os.path.join(dir, label, filename)\n",
        "\n",
        "        if not os.path.isfile(image_fp) or do_override:\n",
        "            pil_im = Image.fromarray(image.astype(np.uint8))\n",
        "            pil_im.save(image_fp)\n",
        "\n",
        "\n",
        "def make_dataset(pickle_path, dir):\n",
        "    raw = unpickle(pickle_path)\n",
        "    x = np.array([data_to_img(img_data) for img_data in raw[b\"data\"]])\n",
        "    y = np.array(raw[b\"labels\"])\n",
        "    pp(x.shape, y.shape)\n",
        "\n",
        "    labels = np.array(meta[b\"label_names\"], dtype=str)[y]\n",
        "    filenames = np.array(raw[b\"filenames\"], dtype=str)\n",
        "\n",
        "    save_images(x, dir, labels, filenames, target_size=None, do_override=False)\n",
        "\n",
        "\n",
        "train_dp = os.path.join(dataset_dp, \"cifar-10_32x32/train/\")\n",
        "test_dp = os.path.join(dataset_dp, \"cifar-10_32x32/test/\")\n",
        "\n",
        "if do_make_dataset:\n",
        "    # Download and extract raw data\n",
        "    raw_data_fp = \"cifar-10-python.tar.gz\"\n",
        "    !cd $dataset_dp && wget -nc https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz -O $raw_data_fp\n",
        "    !cd $dataset_dp && tar --skip-old-files -xf $raw_data_fp\n",
        "\n",
        "    # Find top-level directory(-ies) of an archive.\n",
        "    raw_data_dp = subprocess.check_output(\n",
        "        \"cd {} && tar -tf {} | sed -e 's@/.*@@' | uniq\".format(dataset_dp, raw_data_fp), \n",
        "        shell=True,\n",
        "    )\n",
        "    raw_data_dp = raw_data_dp.decode().replace(\"\\n\", \"\")\n",
        "    # Raw data dir path\n",
        "    raw_data_dp = os.path.join(dataset_dp, raw_data_dp)\n",
        "\n",
        "    # Make dataset\n",
        "    meta = unpickle(os.path.join(raw_data_dp, \"batches.meta\"))\n",
        "\n",
        "    train_pickle_paths = [\n",
        "        os.path.join(raw_data_dp, \"data_batch_1\"),\n",
        "        os.path.join(raw_data_dp, \"data_batch_2\"),\n",
        "        os.path.join(raw_data_dp, \"data_batch_3\"),\n",
        "        os.path.join(raw_data_dp, \"data_batch_4\"),\n",
        "        os.path.join(raw_data_dp, \"data_batch_5\"),\n",
        "    ]\n",
        "    test_pickle_paths = [os.path.join(raw_data_dp, \"test_batch\")]\n",
        "\n",
        "    os.makedirs(train_dp, exist_ok=True)\n",
        "    os.makedirs(test_dp, exist_ok=True)\n",
        "\n",
        "    for cls in meta[b\"label_names\"]:\n",
        "        cls = cls.decode(\"utf-8\")\n",
        "        os.makedirs(os.path.join(train_dp, cls), exist_ok=True)\n",
        "        os.makedirs(os.path.join(test_dp, cls), exist_ok=True)\n",
        "    \n",
        "    for train_pickle_path in train_pickle_paths:\n",
        "        pp(train_pickle_path, train_dp)\n",
        "        make_dataset(train_pickle_path, train_dp)\n",
        "    \n",
        "    for test_pickle_path in test_pickle_paths:\n",
        "        pp(test_pickle_path, test_dp)\n",
        "        make_dataset(test_pickle_path, test_dp)\n",
        "\n",
        "!cd $dataset_dp && pwd && ls -lh && du -h\n",
        "\n",
        "# CPU times: user 6min 41s, sys: 26.9 s, total: 7min 8s\n",
        "# Wall time: 6min 37s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-08 04:32:58--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  31.2MB/s    in 5.9s    \n",
            "\n",
            "2020-12-08 04:33:05 (27.7 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "04:33:10.75 LOG:\n",
            "04:33:10.76 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/data_batch_1'\n",
            "04:33:10.76 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/train/'\n",
            "04:33:15.47 LOG:\n",
            "04:33:15.47 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:33:15.47 .... <argument 2> = (10000,)\n",
            "10000it [00:04, 2142.30it/s]\n",
            "04:33:20.15 LOG:\n",
            "04:33:20.15 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/data_batch_2'\n",
            "04:33:20.15 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/train/'\n",
            "04:33:25.06 LOG:\n",
            "04:33:25.06 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:33:25.06 .... <argument 2> = (10000,)\n",
            "10000it [00:04, 2491.38it/s]\n",
            "04:33:29.09 LOG:\n",
            "04:33:29.09 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/data_batch_3'\n",
            "04:33:29.09 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/train/'\n",
            "04:33:34.37 LOG:\n",
            "04:33:34.37 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:33:34.37 .... <argument 2> = (10000,)\n",
            "10000it [00:03, 2556.18it/s]\n",
            "04:33:38.30 LOG:\n",
            "04:33:38.30 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/data_batch_4'\n",
            "04:33:38.30 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/train/'\n",
            "04:33:43.73 LOG:\n",
            "04:33:43.73 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:33:43.73 .... <argument 2> = (10000,)\n",
            "10000it [00:03, 2516.07it/s]\n",
            "04:33:47.71 LOG:\n",
            "04:33:47.72 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/data_batch_5'\n",
            "04:33:47.72 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/train/'\n",
            "04:33:52.87 LOG:\n",
            "04:33:52.87 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:33:52.88 .... <argument 2> = (10000,)\n",
            "10000it [00:04, 2479.32it/s]\n",
            "04:33:56.93 LOG:\n",
            "04:33:56.93 .... <argument 1> = '/root/datasets/cifar-10/cifar-10-batches-py/test_batch'\n",
            "04:33:56.93 .... <argument 2> = '/root/datasets/cifar-10/cifar-10_32x32/test/'\n",
            "04:34:01.90 LOG:\n",
            "04:34:01.91 .... <argument 1> = (10000, 32, 32, 3)\n",
            "04:34:01.91 .... <argument 2> = (10000,)\n",
            "10000it [00:04, 2489.01it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/root/datasets/cifar-10\n",
            "total 163M\n",
            "drwxr-xr-x 2 2156 1103 4.0K Jun  4  2009 cifar-10-batches-py\n",
            "-rw-r--r-- 1 root root 163M Jun  4  2009 cifar-10-python.tar.gz\n",
            "drwxr-xr-x 4 root root 4.0K Dec  8 04:33 cifar-10_32x32\n",
            "4.0M\t./cifar-10_32x32/test/horse\n",
            "4.0M\t./cifar-10_32x32/test/ship\n",
            "4.0M\t./cifar-10_32x32/test/frog\n",
            "4.0M\t./cifar-10_32x32/test/automobile\n",
            "4.0M\t./cifar-10_32x32/test/dog\n",
            "4.0M\t./cifar-10_32x32/test/bird\n",
            "4.0M\t./cifar-10_32x32/test/deer\n",
            "4.0M\t./cifar-10_32x32/test/truck\n",
            "4.0M\t./cifar-10_32x32/test/airplane\n",
            "4.0M\t./cifar-10_32x32/test/cat\n",
            "40M\t./cifar-10_32x32/test\n",
            "20M\t./cifar-10_32x32/train/horse\n",
            "20M\t./cifar-10_32x32/train/ship\n",
            "20M\t./cifar-10_32x32/train/frog\n",
            "20M\t./cifar-10_32x32/train/automobile\n",
            "20M\t./cifar-10_32x32/train/dog\n",
            "20M\t./cifar-10_32x32/train/bird\n",
            "20M\t./cifar-10_32x32/train/deer\n",
            "20M\t./cifar-10_32x32/train/truck\n",
            "20M\t./cifar-10_32x32/train/airplane\n",
            "20M\t./cifar-10_32x32/train/cat\n",
            "198M\t./cifar-10_32x32/train\n",
            "238M\t./cifar-10_32x32\n",
            "178M\t./cifar-10-batches-py\n",
            "578M\t.\n",
            "CPU times: user 51.1 s, sys: 3.69 s, total: 54.8 s\n",
            "Wall time: 1min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UfqmP7ORNxl"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVphHHL1Wzys",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "%%time\n",
        "# Noted: GPU Tensor Cores (XLA) requires units, filters to be a multiple of 8.\n",
        "\n",
        "# tensorflow_hub\n",
        "def cache_tfhub_model(tfhub_cache_dir, hub_model_link, net_name):\n",
        "    \"\"\"\n",
        "    Manually cache tfhub model, \n",
        "    use this in case automatically caching fail, especially when on Colab.\n",
        "    \"\"\"\n",
        "    tfhub_module_dir = os.path.join(tfhub_cache_dir, net_name)\n",
        "\n",
        "    if not os.path.isfile(os.path.join(tfhub_module_dir, \"saved_model.pb\")):\n",
        "        os.makedirs(tfhub_module_dir, exist_ok=True)\n",
        "        subprocess.run(\n",
        "            f\"curl -L {hub_model_link}?tf-hub-format=compressed | tar -zxvC {tfhub_module_dir}\",\n",
        "            shell=True,\n",
        "            check=True,\n",
        "        )\n",
        "\n",
        "    return tfhub_module_dir\n",
        "\n",
        "\n",
        "# hub_model_link = \"https://tfhub.dev/google/imagenet/mobilenet_v2_100_96/feature_vector/4\"\n",
        "# hub_model_link = \"https://tfhub.dev/google/imagenet/mobilenet_v2_075_96/feature_vector/4\"\n",
        "\n",
        "hub_model_link = \"https://tfhub.dev/google/bit/m-r50x1/1\"\n",
        "# preprocessing_function = lambda x : tf.image.convert_image_dtype(x, tf.float32)\n",
        "net_name = \"bit_m-r50x1\"\n",
        "\n",
        "handle = hub_model_link\n",
        "# handle = cache_tfhub_model(tfhub_cache_dir, hub_model_link, net_name)\n",
        "# << tensorflow_hub\n",
        "\n",
        "\n",
        "# # tf.keras.applications\n",
        "# # base_model = tk.applications.MobileNetV2\n",
        "# # preprocessing_function = tk.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# # base_model = tk.applications.EfficientNetB0\n",
        "# # base_model = tk.applications.EfficientNetB4\n",
        "# # preprocessing_function = tk.applications.efficientnet.preprocess_input\n",
        "\n",
        "# # base_model = tk.applications.InceptionV3\n",
        "# # preprocessing_function = tk.applications.inception_v3.preprocess_input\n",
        "\n",
        "# base_model = tf.keras.applications.Xception\n",
        "# preprocessing_function = tk.applications.xception.preprocess_input\n",
        "\n",
        "# # base_model = tf.keras.applications.InceptionResNetV2\n",
        "# # preprocessing_function = tk.applications.inception_resnet_v2.preprocess_input\n",
        "\n",
        "# # base_model = tf.keras.applications.ResNet50V2\n",
        "# # preprocessing_function = tk.applications.resnet_v2.preprocess_input\n",
        "\n",
        "# # base_model = tf.keras.applications.NASNetLarge\n",
        "# # base_model = tf.keras.applications.NASNetMobile\n",
        "# # preprocessing_function = tk.applications.nasnet.preprocess_input\n",
        "\n",
        "# net_name = base_model.__name__\n",
        "# # << tf.keras.applications\n",
        "\n",
        "\n",
        "def get_model(input_shape, n_classes):\n",
        "    # tf.keras.applications\n",
        "    # base = base_model(\n",
        "    #     include_top=False,\n",
        "    #     weights=\"imagenet\",\n",
        "    #     pooling=\"avg\",\n",
        "    #     name=net_name,\n",
        "    # )\n",
        "    # tensorflow_hub\n",
        "    base = hub.KerasLayer(handle, name=net_name, trainable=True)\n",
        "\n",
        "    # base.trainable = True\n",
        "    # for layer in base.layers:\n",
        "    #     if isinstance(layer, tk.layers.BatchNormalization):\n",
        "    #         layer.trainable = False\n",
        "\n",
        "    inputs = tk.layers.Input(input_shape)\n",
        "    x = base(inputs)\n",
        "    x = tk.layers.Dense(n_classes, kernel_initializer=\"zeros\", name=\"logits\")(x)\n",
        "    outputs = tk.layers.Activation(\"softmax\", name=\"pred\", dtype=tf.float32)(x)  # Use \"float32\" for numeric stability.\n",
        "\n",
        "    model = tk.models.Model(inputs, outputs, name=f\"{ds_name}_{net_name}\")\n",
        "    print(\"Layers' computations dtype \", x.dtype)\n",
        "    print(\"Outputs' dtype \", outputs.dtype)\n",
        "    return model\n",
        "\n",
        "\n",
        "reset_env()\n",
        "model = get_model(input_shape, n_classes)\n",
        "\n",
        "model_plot_fp = f\"{ds_name}_{net_name}.png\"\n",
        "tk.utils.plot_model(model, show_shapes=True, to_file=model_plot_fp)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlCHBwGPREdk"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxdBGeyzWbbi",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "common_generator_cfg = dict(\n",
        "    rescale=1/255.,\n",
        "    # preprocessing_function=preprocessing_function,\n",
        "    dtype=tf.float32,  # On CPU, float32 operations are faster.\n",
        ")\n",
        "\n",
        "data_aug_cfg = dict(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    # shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode=\"nearest\",\n",
        "    horizontal_flip=True,\n",
        ") if do_augmentation else {}\n",
        "\n",
        "flow_cfg = dict(\n",
        "    target_size=input_shape[:2],\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "\n",
        "train_gen = ImageDataGenerator(**{**common_generator_cfg, **data_aug_cfg})\n",
        "train_data = train_gen.flow_from_directory(train_dp, shuffle=True, **flow_cfg)\n",
        "\n",
        "test_gen = ImageDataGenerator(**common_generator_cfg)\n",
        "test_data = test_gen.flow_from_directory(test_dp, shuffle=False, **flow_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D4f1L7lRC4Q"
      },
      "source": [
        "# HP Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR1ABne6RIXf",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "class DecayType(enum.IntEnum):\n",
        "    \"\"\"Data class, each decay type is assigned a number.\"\"\"\n",
        "    LINEAR = 0\n",
        "    COSINE = 1\n",
        "    EXPONENTIAL = 2\n",
        "    POLYNOMIAL = 3\n",
        "\n",
        "\n",
        "class DecayScheduler():\n",
        "    \"\"\"Given initial and endvalue, \n",
        "    this class generates the value depending on decay type and decay steps (by calling).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, start_val, end_val, decay_steps, decay_type, extra=1.0):\n",
        "        self.start_val = start_val\n",
        "        self.end_val = end_val\n",
        "        self.decay_steps = decay_steps\n",
        "        self.decay_type = decay_type\n",
        "        self.extra = extra\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        if self.decay_type == DecayType.LINEAR:\n",
        "            pct = step / self.decay_steps\n",
        "            return self.start_val + pct * (self.end_val - self.start_val)\n",
        "        elif self.decay_type == DecayType.COSINE:\n",
        "            cos_out = np.cos(np.pi * step / self.decay_steps) + 1\n",
        "            return self.end_val + (self.start_val - self.end_val) / 2 * cos_out\n",
        "        elif self.decay_type == DecayType.EXPONENTIAL:\n",
        "            ratio = self.end_val / self.start_val\n",
        "            return self.start_val * ratio **  (step / self.decay_steps)\n",
        "        elif self.decay_type == DecayType.POLYNOMIAL:\n",
        "            return self.end_val + (self.start_val - self.end_val) * (1 - step / self.decay_steps) ** self.extra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A58rqOU27XsT",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "def get_decay_steps(cycle_len, anneal_pct):\n",
        "    phase_len = int(cycle_len * (1 - anneal_pct) / 2)\n",
        "    anneal_len = cycle_len - phase_len * 2\n",
        "    return phase_len, phase_len, anneal_len\n",
        "\n",
        "def onecyle_learning_rate(\n",
        "    init_lr,\n",
        "    end_lr,\n",
        "    train_steps,\n",
        "    decay_type=DecayType.LINEAR,\n",
        "    anneal_pct=0.075,\n",
        "):\n",
        "    \"\"\"OneCyle learning rates\n",
        "    Args:\n",
        "        anneal_pct (float): \n",
        "            Percentage to leave for the annealing at the end.\n",
        "            The annealing phase goes from the minimum lr to 1/100th of it linearly.\n",
        "    \"\"\"\n",
        "    phase_decay_steps = get_decay_steps(train_steps, anneal_pct)\n",
        "\n",
        "    lr_schedulers = [\n",
        "        DecayScheduler(init_lr, end_lr, phase_decay_steps[0], decay_type),\n",
        "        DecayScheduler(end_lr, init_lr, phase_decay_steps[1], decay_type),\n",
        "        DecayScheduler(init_lr, init_lr / 100., phase_decay_steps[2], decay_type),\n",
        "    ]\n",
        "\n",
        "    learning_rates = []\n",
        "    for lr_scheduler, decay_steps in zip(lr_schedulers, phase_decay_steps):\n",
        "        learning_rates.append(lr_scheduler(np.arange(decay_steps)))\n",
        "\n",
        "    learning_rates = np.concatenate(learning_rates, 0)\n",
        "    return learning_rates\n",
        "\n",
        "\n",
        "def onecyle_momentum(\n",
        "    init_mom,\n",
        "    end_mom,\n",
        "    train_steps,\n",
        "    decay_type=DecayType.LINEAR,\n",
        "    anneal_pct=0.075,\n",
        "):\n",
        "    \"\"\"OneCyle learning rates\n",
        "    Args:\n",
        "        anneal_pct (float): \n",
        "            Percentage to leave for the annealing at the end.\n",
        "            The annealing phase use constant maximum momentum.\n",
        "    \"\"\"\n",
        "    phase_decay_steps = get_decay_steps(train_steps, anneal_pct)\n",
        "\n",
        "    mom_schedulers = [\n",
        "        DecayScheduler(init_mom, end_mom, phase_decay_steps[0], decay_type),\n",
        "        DecayScheduler(end_mom, init_mom, phase_decay_steps[1], decay_type),\n",
        "    ]\n",
        "\n",
        "    moms = []\n",
        "    for mom_scheduler, decay_steps in zip(mom_schedulers, phase_decay_steps):\n",
        "        moms.append(mom_scheduler(np.arange(decay_steps)))\n",
        "    moms.append(np.array([init_mom] * phase_decay_steps[2]))\n",
        "\n",
        "    moms = np.concatenate(moms, 0)\n",
        "    return moms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ov6UfaQvqSM",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "class OneCycleScheduler(tk.callbacks.Callback):\n",
        "    \"\"\"Callback that update lr, momentum at begining of mini batch based on OneCycle policy.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        init_lr,\n",
        "        end_lr,\n",
        "        train_steps,\n",
        "        decay_type=DecayType.LINEAR,\n",
        "        anneal_pct=0.075,\n",
        "        init_mom=None,\n",
        "        end_mom=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.train_steps = train_steps\n",
        "\n",
        "        common_kwargs = dict(\n",
        "            train_steps=train_steps,\n",
        "            decay_type=decay_type,\n",
        "            anneal_pct=anneal_pct,\n",
        "        )\n",
        "\n",
        "        self.learning_rates = onecyle_learning_rate(\n",
        "            init_lr=init_lr,\n",
        "            end_lr=end_lr,\n",
        "            **common_kwargs\n",
        "        )\n",
        "\n",
        "        if not (init_mom is None or end_mom is None):\n",
        "            self.moms = onecyle_momentum(\n",
        "                init_mom=init_mom,\n",
        "                end_mom=end_mom,\n",
        "                **common_kwargs\n",
        "            )\n",
        "        else:\n",
        "            self.moms = None\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.train_step = 0\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        if self.train_step < self.train_steps:\n",
        "            K.set_value(self.model.optimizer.learning_rate, self.learning_rates[self.train_step])\n",
        "\n",
        "            if not self.moms is None:\n",
        "                # If using \"tf.keras.mixed_precision.experimental\", then set HP this way (except for learning_rate)\n",
        "                # K.set_value(self.model.optimizer._optimizer.beta_1, self.moms[self.train_step])\n",
        "\n",
        "                K.set_value(self.model.optimizer.beta_1, self.moms[self.train_step])\n",
        "\n",
        "        self.train_step += 1\n",
        "\n",
        "    def plot(self):\n",
        "        a_train_steps = np.arange(self.train_steps)\n",
        "        traces = [\n",
        "            go.Scatter(x=a_train_steps, y=self.learning_rates, name=\"learning_rate\"),\n",
        "        ]\n",
        "        if not self.moms is None:\n",
        "            traces.append(go.Scatter(x=a_train_steps, y=self.moms, name=\"momentum\"))\n",
        "\n",
        "        fig = make_subplots(len(traces), 1)\n",
        "        for i, trace in enumerate(traces):\n",
        "            fig.add_trace(trace, row=i+1, col=1)\n",
        "        fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE3VQBLS8Ysl"
      },
      "source": [
        "# LR Finder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WFER7dzoQ91",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "class LRFinder(tk.callbacks.Callback):\n",
        "    \"\"\"Learning Rate Finder Callback\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        init_lr,\n",
        "        end_lr,\n",
        "        decay_type=DecayType.EXPONENTIAL,\n",
        "        beta=0.98\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.init_lr = init_lr\n",
        "        self.end_lr = end_lr\n",
        "        self.decay_type = decay_type\n",
        "        self.beta = beta\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        # pp(self.params)\n",
        "        self.train_steps = self.params[\"epochs\"] * self.params[\"steps\"]\n",
        "\n",
        "        self.scheduler = DecayScheduler(\n",
        "            start_val=self.init_lr, \n",
        "            end_val=self.end_lr, \n",
        "            decay_steps=self.train_steps,\n",
        "            decay_type=self.decay_type, \n",
        "        )\n",
        "        self.learning_rates = self.scheduler(np.arange(self.train_steps))\n",
        "\n",
        "        self.history = {}\n",
        "        self.train_step = 0\n",
        "        self.avg_loss = 0.\n",
        "        self.best_loss = 0.\n",
        "        self.best_learning_rate = 0.\n",
        "\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        K.set_value(self.model.optimizer.learning_rate, self.learning_rates[self.train_step])\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        # Compute the smoothed loss\n",
        "        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * logs[\"loss\"]\n",
        "        smoothed_loss = self.avg_loss / (1 - self.beta ** (self.train_step + 1))\n",
        "\n",
        "        # Stop if the loss is exploding\n",
        "        if self.train_step > 1 and smoothed_loss > 4 * self.best_loss:\n",
        "            self.model.stop_training = True\n",
        "            print(\"Stop training because loss is exploding.\")\n",
        "\n",
        "        # Record the best loss, learning_rate\n",
        "        if self.train_step == 1 or smoothed_loss < self.best_loss:\n",
        "            self.best_loss = smoothed_loss\n",
        "            self.best_learning_rate = self.learning_rates[self.train_step]\n",
        "\n",
        "        if not self.model.stop_training:\n",
        "            # History\n",
        "            for key, val in logs.items():\n",
        "                self.history.setdefault(key, []).append(val)\n",
        "            self.history.setdefault(\"smoothed_loss\", []).append(smoothed_loss)\n",
        "\n",
        "            self.train_step += 1\n",
        "\n",
        "    def plot_learning_rate(self):\n",
        "        fig = px.line(\n",
        "            x=np.arange(self.train_step),\n",
        "            y=self.learning_rates[:self.train_step],\n",
        "            labels=dict(x=\"train_step\", y=\"learning_rate\"),\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def _plot_metric(self, metric, metric_name):\n",
        "        fig = px.line(\n",
        "            x=self.learning_rates[:self.train_step],\n",
        "            y=metric, \n",
        "            log_x=True,\n",
        "            labels=dict(x=\"learning_rate(log)\", y=metric_name),\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def plot_accuracy(self):\n",
        "        self._plot_metric(self.history[\"accuracy\"], \"accuracy\")\n",
        "\n",
        "    def plot_loss(self):\n",
        "        self._plot_metric(self.history[\"loss\"], \"loss\")\n",
        "\n",
        "    def plot_smoothed_loss(self):\n",
        "        self._plot_metric(self.history[\"smoothed_loss\"], \"smoothed_loss\")\n",
        "\n",
        "\n",
        "def plot_metric(learning_rates, metrics, metric_name, optimizer_cfgs):\n",
        "    \"\"\"\n",
        "    Plot the same metric from multiple training run (using different optimizer configuration),\n",
        "    against learning rates.\n",
        "    \"\"\"\n",
        "    fig = go.Figure()\n",
        "\n",
        "    for metric, optimizer_cfg in zip(metrics, optimizer_cfgs):\n",
        "        if len(metric) < len(learning_rates):\n",
        "            metric = np.append(\n",
        "                metric, [np.nan] * (len(learning_rates) - len(metric))\n",
        "            )\n",
        "        fig.add_trace(\n",
        "            go.Scatter(x=learning_rates, y=metric, name=f\"{optimizer_cfg}\")\n",
        "        )\n",
        "\n",
        "    fig.update_xaxes(type=\"log\")\n",
        "    fig.update_layout(xaxis_title=\"learning_rate(log)\", yaxis_title=metric_name)\n",
        "    fig.show()\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqIGsU9lZV6S",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "loss = \"categorical_crossentropy\"\n",
        "\n",
        "optimizer_class = tfa.optimizers.AdamW\n",
        "weight_decays = [1e-2, 1e-3, 1e-4]\n",
        "optimizer_cfgs = [\n",
        "    dict(weight_decay=wd) for wd in weight_decays\n",
        "]\n",
        "\n",
        "# optimizer_class = tk.optimizers.RMSprop\n",
        "# optimizer_class = tk.optimizers.Adam\n",
        "# optimizer_cfg = dict()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dY_jKl59oQ91",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "%%time\n",
        "if do_find_lr:\n",
        "    init_lr = 1e-4\n",
        "    end_lr = 1\n",
        "\n",
        "    learning_rates = None\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    smoothed_losses = []\n",
        "    best_losses = []\n",
        "    best_learning_rates = []\n",
        "\n",
        "    for optimizer_cfg in optimizer_cfgs:\n",
        "        pp(optimizer_cfg)\n",
        "        \n",
        "        reset_env()\n",
        "        model = get_model(input_shape, n_classes)\n",
        "\n",
        "        cb_lrfinder = LRFinder(init_lr, end_lr, decay_type=DecayType.EXPONENTIAL, beta=0.98)\n",
        "\n",
        "        optimizer = optimizer_class(**optimizer_cfg)\n",
        "        model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "        model.fit(\n",
        "            train_data,\n",
        "            epochs=1, \n",
        "            batch_size=batch_size, \n",
        "            callbacks=[cb_lrfinder], \n",
        "            verbose=1,\n",
        "            **prefetch_cfg\n",
        "        )\n",
        "\n",
        "        # cb_lrfinder.plot_learning_rate()\n",
        "        # cb_lrfinder.plot_accuracy()\n",
        "        # cb_lrfinder.plot_loss()\n",
        "        # cb_lrfinder.plot_smoothed_loss()\n",
        "        # pp(cb_lrfinder.best_loss, cb_lrfinder.best_learning_rate)\n",
        "\n",
        "        if learning_rates is None:\n",
        "            learning_rates = cb_lrfinder.learning_rates\n",
        "\n",
        "        accuracies.append(cb_lrfinder.history[\"accuracy\"])\n",
        "        losses.append(cb_lrfinder.history[\"loss\"])\n",
        "        smoothed_losses.append(cb_lrfinder.history[\"smoothed_loss\"])\n",
        "        best_losses.append(cb_lrfinder.best_loss)\n",
        "        best_learning_rates.append(cb_lrfinder.best_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1tLGpqwuETG",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "lr_find_result_f = os.path.join(lr_find_result_dir, f\"{net_name}.pickle\")\n",
        "\n",
        "if do_find_lr:\n",
        "    lr_find_result = {\n",
        "        \"accuracies\" : accuracies,\n",
        "        \"losses\" : losses,\n",
        "        \"smoothed_losses\" : smoothed_losses,\n",
        "        \"learning_rates\" : learning_rates,\n",
        "        \"best_losses\" : best_losses,\n",
        "        \"best_learning_rates\": best_learning_rates,\n",
        "    }\n",
        "    with open(lr_find_result_f, \"wb\") as pickle_fo:\n",
        "        pickle.dump(lr_find_result, pickle_fo)\n",
        "else:\n",
        "    if os.path.isfile(lr_find_result_f):\n",
        "        with open(lr_find_result_f, \"rb\") as pickle_fo:\n",
        "            lr_find_result = pickle.load(pickle_fo)\n",
        "    else:\n",
        "        lr_find_result = {}        \n",
        "\n",
        "for key, val in lr_find_result.items():\n",
        "    if key in [\"accuracies\", \"losses\", \"smoothed_losses\"]:\n",
        "        fig = plot_metric(\n",
        "            lr_find_result[\"learning_rates\"],\n",
        "            metrics=val,\n",
        "            metric_name=key,\n",
        "            optimizer_cfgs=optimizer_cfgs\n",
        "        )\n",
        "\n",
        "pp(optimizer_cfgs, lr_find_result.get(\"best_losses\"), lr_find_result.get(\"best_learning_rates\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3-147gd3FtV"
      },
      "source": [
        "# HP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Te0_bkJQFrs",
        "trusted": true,
        "scrolled": true
      },
      "source": [
        "n_batches_per_epoch = len(train_data)\n",
        "n_train_steps = n_epochs * n_batches_per_epoch\n",
        "pp(n_train_steps)\n",
        "\n",
        "optimizer_cfg = dict(\n",
        "#     weight_decay=0.0001,  # bit_m-r50x fine tune 512 batch_size, 30 epochs\n",
        "    weight_decay=0.0001,  # bit_m-r50x fine tune 128 batch_size, 18 epochs\n",
        ")\n",
        "\n",
        "# LR\n",
        "# end_lr = 0.001  # bit_m-r50x fine tune 512 batch_size, 30 epochs\n",
        "end_lr = 0.001  # bit_m-r50x fine tune 128 batch_size, 18 epochs\n",
        "init_lr = end_lr / 10.\n",
        "\n",
        "# Momemtum\n",
        "init_mom = 0.95\n",
        "end_mom = 0.85\n",
        "\n",
        "cb_onecycle = OneCycleScheduler(\n",
        "    init_lr, \n",
        "    end_lr,\n",
        "    n_train_steps,\n",
        "    decay_type=DecayType.LINEAR,\n",
        "    anneal_pct=0.075,\n",
        "    init_mom=init_mom,\n",
        "    end_mom=end_mom,\n",
        ")\n",
        "cb_onecycle.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mORDm9zERLBp"
      },
      "source": [
        "# Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkT6kliMnm0M",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "monitor = \"val_accuracy\"\n",
        "mode = \"max\"\n",
        "# goal = 0.97\n",
        "min_delta = 1e-3\n",
        "\n",
        "validation_freq = 3\n",
        "\n",
        "cb_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=monitor,\n",
        "    mode=mode,\n",
        "    min_delta=min_delta,\n",
        "    patience=9,\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "# Model checkpoint dir\n",
        "model_root_dir = os.path.join(\"models/\", net_name)\n",
        "os.makedirs(model_root_dir, exist_ok=True)\n",
        "\n",
        "checkpoint_fp = os.path.join(model_root_dir, now_str)\n",
        "model_fps = os.listdir(model_root_dir)\n",
        "lastest_model_fp = (\n",
        "    os.path.join(model_root_dir, sorted(model_fps)[-1]) \n",
        "    if len(model_fps) > 0 \n",
        "    else None\n",
        ")\n",
        "\n",
        "cb_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_fp,\n",
        "    monitor=monitor,\n",
        "    mode=mode,\n",
        "    save_weights_only=False,\n",
        "    save_best_only=True,\n",
        "    save_format=\"tf\",\n",
        "    include_optimizer=True,\n",
        ")\n",
        "\n",
        "# Tensorboard logs dir\n",
        "tb_logs_root_dir = os.path.join(\"tb_logs/\", net_name)\n",
        "if do_train:\n",
        "    tb_logs_dp = os.path.join(tb_logs_root_dir, now_str)\n",
        "else:\n",
        "    tb_logs_dps = os.listdir(tb_logs_root_dir)\n",
        "    tb_logs_dp = (\n",
        "        os.path.join(tb_logs_root_dir, sorted(tb_logs_dps)[-1]) \n",
        "        if len(tb_logs_dps) > 0 \n",
        "        else None\n",
        "    )\n",
        "\n",
        "cb_tensorboard = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=tb_logs_dp,\n",
        "    histogram_freq=1,\n",
        "    update_freq='epoch',\n",
        "    profile_batch=\"2,22\",\n",
        ")\n",
        "\n",
        "\n",
        "# class Goal(tf.keras.callbacks.Callback):\n",
        "#     def __init__(self, monitor, mode, goal):\n",
        "#         super().__init__()\n",
        "#         self.monitor = monitor\n",
        "#         self.mode = mode\n",
        "#         self.goal = goal\n",
        "\n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         if self.mode == \"min\":\n",
        "#             goal_achieved = logs[self.monitor] <= self.goal\n",
        "#         elif self.mode == \"max\":\n",
        "#             goal_achieved = logs[self.monitor] >= self.goal\n",
        "\n",
        "#         if goal_achieved:\n",
        "#             print(\"Goal {}: {} achieved. Stop training.\".format(self.monitor, self.goal))\n",
        "#             self.model.stop_training = True\n",
        "\n",
        "\n",
        "# cb_goal = Goal(monitor, mode, goal)\n",
        "\n",
        "# cb_reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "#     monitor=monitor,\n",
        "#     mode=mode,\n",
        "#     min_delta=min_delta,\n",
        "#     patience=5,\n",
        "#     factor=0.2,\n",
        "#     min_lr=1e-6,\n",
        "# )\n",
        "\n",
        "callbacks = [\n",
        "    cb_checkpoint,\n",
        "    cb_early_stopping,\n",
        "    cb_tensorboard,\n",
        "    # cb_goal,\n",
        "    # cb_reduce_lr,\n",
        "    cb_onecycle,\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxMYqm3xR0Vr"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTMrnqIinoTK",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "%%time\n",
        "reset_env()\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "# For test run\n",
        "# n_epochs = 3\n",
        "\n",
        "if do_train:\n",
        "\n",
        "    model_loaded = False\n",
        "    if do_load_model:\n",
        "        try:\n",
        "            model = tk.models.load_model(lastest_model_fp, compile=True)\n",
        "            model_loaded = True\n",
        "            print(\"Model loaded: {}\".format(lastest_model_fp))\n",
        "        except Exception as ex:\n",
        "            print(\"ERROR load_model: {}\".format(ex))\n",
        "    \n",
        "    if not do_load_model or not model_loaded:\n",
        "        model = get_model(input_shape, n_classes)\n",
        "        model.compile(\n",
        "            loss=loss,\n",
        "            optimizer=optimizer_class(**optimizer_cfg),\n",
        "            metrics=[\"accuracy\"]\n",
        "        )\n",
        "        print(\"Created new model.\")\n",
        "\n",
        "    history = model.fit(\n",
        "        train_data,\n",
        "        epochs=n_epochs,\n",
        "        validation_data=test_data,\n",
        "        validation_freq=validation_freq,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1,\n",
        "        **prefetch_cfg,\n",
        "    )\n",
        "else:\n",
        "    model = tk.models.load_model(lastest_model_fp, compile=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZHWJ6pE3IWw"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aS58GcroUPt",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "model.evaluate(test_data, verbose=1, **prefetch_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPSzNbYFdeQs",
        "trusted": true,
        "scrolled": false
      },
      "source": [
        "%tensorboard --logdir $tb_logs_dp"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}